{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a AI for 2048\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Kavin Govinda Raj\n",
    "- Hyun Woo Choi\n",
    "- Micheal Daoud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "Our goal is to make an effective AI that can solve the App game 2048. The goal is for the model to reach 2048 every time, but we also plan on seeing how far our model can go. We will be comparing different AI algorithms with each other. We plan on collecting game data, we can make a scoring system/ highest number achieved to determine how well the different AIs are performing. We plan on collecting data from about 100 runs of our methods and seeing which ones performed well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The game 2048 is a 4x4 grid game that randomly generates the numbers 2 and 4 whenever a user does an action. There are four actions which include swiping up, down, left, and right. When swiping in a direction all of the numbers in the grid would move in that direction until it hits the border or another number. You can combine these numbers during an action if they are the same and once combined it goes up by that multiple. EX. 2 and 2 become 4 and 4 and 4 becomes 16 and so on. To beat the game you must be able to combine these numbers within the 4x4 grid to get 2048 or above. If the entire grid is filled and there are no possible combines that can be done then the game is over. Our goal is to utilize various algorithms to report which one performs best in 2048. Many 2048 AIs already exist. An example found was a paper published from Stanford.<a name=\"nie\"></a>[<sup>[1]</sup>](#nienote) They utilized Minimax, Expectimax, and reinforcment learning. They noticed a higher success rate with Expectimax. We plan to implement Expectimax. We hope to get similar results as the Stanford paper. There is a pygame library that includes 2048.<a name=\"banerjee\"></a>[<sup>[2]</sup>](#banerjeenote). The plan is to add our implementation with this already-provided game file. There is also a paper talking about utilizing the A* search algorithm on the 8-Puzzle game.<a name=\"sonawane\"></a>[<sup>[3]</sup>](#sonawanenote) This paper saw success with a search algorithm with a similar game. So the plan is to use these three algorithms and compare them: Expectimax, A* Search, and Reinforcement Learning. We think this is a great start and we could add more algorithms like greedy for example if the time persists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our problem is to be able to solve 2048. For that, the algorithm must be able to combine numbers and efficiently hit the goal of 2048 to ensure it doesn’t fill up the entire board and lose. To address this challenge, we have identified several sub-goals: effectively merging tiles to maximize board space, predicting and planning future moves, and optimizing tile placement for high-value combinations. We will model the game states using a 4x4 matrix of integers, each representing a tile's value. Our approach will explore various algorithms, including A*, Monte Carlo Simulations, Q-learning, and Expectimax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We don't plan on using any data outside the game, but we plan on collecting data from the game which we will later compare. The data we are collecting will include the highest score out of all games and the average of all scores.\n",
    "\n",
    "- Expectimax: [1916, 1062, 1518, 1030, 924, 1122, 2110, 798, 1090, 1160, 712, 820, 2110, 602, 1602, 1558, 1302, 1700, 1618, 1058, 990, 1576, 1724, 602, 1378, 1306, 978, 2332, 2524, 1920, 2106, 1492, 1252, 1344, 1506, 1612, 1524, 1892, 1134, 1150, 1858, 850, 1072, 1126, 1918, 840, 1182, 930, 1762, 902, 804, 1614, 838, 454, 1054, 876, 1338, 1034, 1064, 878, 886, 1012, 1994, 1356, 2164, 934, 1668, 716, 960, 1468, 622, 2018, 404, 612, 826, 1594, 1812, 1978, 594, 1876, 2036, 1732, 738, 1114, 772, 1018, 1516, 864, 830, 626, 1994, 942, 2184, 1098, 1650, 2040, 902, 860, 1096, 1008]\n",
    "- Highest: 2524\n",
    "- Average:1290.62\n",
    "\n",
    "- Random: [230, 306, 320, 362, 180, 340, 172, 156, 408, 138, 196, 398, 336, 294, 414, 164, 310, 262, 262, 296, 280, 208, 304, 320, 180, 300, 238, 272, 172, 242, 168, 266, 210, 200, 188, 144, 280, 170, 258, 256, 282, 352, 440, 194, 368, 348, 380, 256, 272, 218, 218, 486, 296, 394, 226, 350, 150, 396, 244, 276, 276, 150, 308, 466, 100, 308, 226, 252, 236, 478, 300, 250, 208, 208, 192, 372, 272, 288, 222, 200, 166, 222, 222, 144, 266, 140, 312, 194, 364, 484, 306, 238, 236, 120, 196, 304, 150, 196, 220, 300]\n",
    "- Highest: 486\n",
    "- Average: 264.38\n",
    "\n",
    "- DQN: [414, 330, 228, 326, 440, 160, 202, 338, 390, 218, 222, 312, 192, 296, 210, 326, 252, 272, 146, 178, 428, 348, 220, 330, 526, 222, 290, 306, 230, 246, 324, 192, 194, 254, 218, 228, 184, 208, 352, 220, 198, 282, 268, 156, 210, 304, 274, 212, 366, 240, 214, 236, 226, 276, 206, 350, 304, 250, 310, 342, 200, 176, 274, 202, 140, 192, 152, 288, 220, 314, 316, 132, 222, 350, 182, 260, 186, 240, 310, 304, 206, 238, 258, 278, 166, 226, 190, 186, 278, 480, 312, 258, 120, 282, 316, 188, 182, 398, 334, 206]\n",
    "- Highest: 526\n",
    "- Average: 259.58\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our approach to solving the game of 2048 focuses on two main algorithms: Expectimax and Deep Q-Networks (DQN).\n",
    "\n",
    "- Expectimax: This algorithm is ideal for managing the inherent uncertainty in 2048, where new tiles appear in random locations after each move. Expectimax simulates possible future moves and evaluates the potential outcomes to make informed decisions. By considering the probabilities of tile appearances and their placements, Expectimax can strategically plan moves that maximize the board's potential for high-value combinations.\n",
    "\n",
    "- Deep Q-Networks (DQN): DQN extends traditional Q-learning by using deep neural networks to approximate the Q-function, which evaluates the quality of particular actions in given states. This allows the algorithm to handle the large state space of 2048 more effectively than standard Q-learning, which struggles with the game's high dimensionality. DQN learns optimal strategies by continually adjusting its predictions based on rewards received for actions taken, effectively learning to predict the sequence of moves that leads to the highest scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We plan on measuring multiple things. The main things being noted are what is the highest tile achieved and the total score. The scoring system already implemented in the game records the total value of all added squares while the step count records the number of steps to reach the goal. We ran 100 trials for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "As we started this project the initial algorithms we considered were A*, Monte Carlo Simulations, Q-learning and expected max. After doing more background research and extensive testing, we decided to focus on Expectimax and Deep Q networks while using random moves as a control to test the efficiency of our algorithms.\n",
    "\n",
    "### Expectimax\n",
    "\n",
    "In regards to 2048, Expectimax takes a look at all the moves it can make and takes account of the puzzle’s randomness to find what is the best score. We made it search-in with a depth of 4 for each move, meaning it will be looking 4 moves deep. Before committing to a finalized move it will foresee every possible move while also taking into account every possible empty space a 2 or 4 tile will spawn and see what moves are giving us an average higher score. At first, we only had one heuristic, which was the perfect snake. <a name=\"tinker\"></a><sup>[5]</sup> This was trying to mimic the way a human plays 2048. From the first run, the highest tile we got was 512. This was at a point where random was performing better than anything else. We added alpha-beta pruning for better optimization so it won’t search every node only the one necessary to look into.\n",
    "\n",
    " We decided to continue perfecting the heuristics of Expectimax. We added a few more including Monotonicity, Smoothness, Empty Tiles, and Merge Potential. Monotonicity rewards boards where tiles are increasing or decreasing in order either in rows or columns. Smoothness penalizes large differences between adjacent tiles, which helps a lot in combination with perfect snakes.  Empty Tiles rewards boards with more empty tiles, encouraging more combinations. And merge potential rewards boards when the same numbers are adjacent to each other. \n",
    "\n",
    "We continuously changed and ran different weights for all of these heuristics, seeing very minimal improvements. At noticed that the key heuristics that have been helping us were perfect snake and smoothness in combination. It was able to get higher than 512 on most trails consistently. We added monotonicity and a little weight of empty tiles and found it to get 1024 more consistently. The finalized combination we got was 0.30 for perfect snake, 0.50 for monotonicity, 0.5 for smoothness, and 0.1 for empty tiles. This combination gives us the data we collected earlier in the report. We see that we win the game less than ten percent of the time, but get the highest tile 1024 around 70% of the time.\n",
    "\n",
    "### Deep Q-Network\n",
    "\n",
    "Initially, we attempted to implement the traditional Q-learning algorithm for our 2048 game AI. However, we encountered significant challenges due to the nature of the game. Q-learning tends to perform worse in environments like 2048 because it lacks the capacity for effective future planning—it focuses only on immediate rewards and the subsequent state. Additionally, the state space in 2048 is vast and highly variable; Q-learning struggles here because it must learn a value for each specific state-action pair, leading to a sparse and inefficient learning process. This method does not generalize well across similar states, making it difficult to apply learned strategies to slightly different board configurations.\n",
    "\n",
    "To address these limitations, we implemented a Deep Q-Network (DQN). Unlike traditional Q-learning, DQN utilizes deep neural networks to approximate the Q-value function, which predicts the quality of actions taken from given states. The primary advantage here is the ability of the DQN to generalize from observed states to unseen ones, effectively reducing the dimensionality and complexity of the state space. The DQN learns to evaluate board states and determine the potential long-term rewards of actions, allowing it to plan more strategically and robustly. It does this through continuous updates from experience replay, which stores past transitions and reuses this data to minimize correlations between updates and avoid overfitting. \n",
    "\n",
    "We implemented a DQN architecture similar to the one described in a Medium article. <a name=\"qwert\"></a><sup>[4]</sup>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "In both algorithms the main issue explored in the game is to visualize the game in multiple states and to predict multiple outcomes ahead of time. This is why using a depth search was so effective for expectimax with and why Deep Q being able to learn patterns that traditional Q-learning was unable to learn. In a game like 2048 having the ability to set up and execute  a specific combination or series of combinations is essential to playing the game. Thus the implementation of a deep learning network such as DQN or incorporating deep search in expectimax proved essential.\n",
    "\n",
    "This initial hypothesis is proved to be true as with Q-learning as when testing it the short cited nature of Q-learning forced it to make a series of suboptimal moves. Lacking the vision to see multiple states in the future Q-learning would prioritize making immediate combinations rather than playing the game strategically. With Deep Q having the ability to learn from past games and improve resulting in a higher average than traditional Q-learning. However, we were unable to run enough trials to outperform random.\n",
    "\n",
    "With Expectimax when initially testing we would rarely achieve a tile of 512 or above. However as we implemented depth search and improved the heuristics we were able to get to a highest tile 1024 with regularity and to win the game on occasion. The heuristics provided allowed the algorithm to play the game with what would be ideal strategies while the depth search mimicked the ability of a person to think and plan for multiple future states. This allowed expectimax to perform so well as it was able to adapt traditional methods that people use to solve the game as seen with the snake heuristic while using the policies allowed the AI to outperform what we deemed capable in certain situations. With more time further improvements to the heuristics could possibly be made to allow the algorithm to play in a more dynamic fashion but it still performed exceptionally well.\n",
    "\n",
    "\n",
    "### Limitations \n",
    "\n",
    "Given more time, we could have further refined our approach by conducting more extensive hyperparameter tuning. Deep Q-Networks (DQN) involve a wider range of hyperparameters compared to traditional Q-learning, primarily due to their reliance on neural networks. Additionally, optimizing our reward and penalty strategies could have significantly enhanced the performance of our model.\n",
    "\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "In addressing the ethical and privacy aspects of our AI agent project for the game 2048, we have ensured compliance with all relevant software licenses. The underlying game code from Pygame 2048 is covered under an MIT license, which we respect by retaining the license file in our project repository. Ethically, we focus on the transparency and originality of our work, especially concerning the implementation of algorithms. To prevent plagiarism and ensure the integrity of our contributions, we meticulously document our sources and modifications, adhering to best practices in code attribution.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "2048 is a game of trying to combine numbers in the power of 2 in an attempt to get a maximum score. There are many AI algorithms out there equipped to find the maximum score, and with the experiments we conducted, it has been seen that Expectimax has performed the best. With  more time to refine the heuristics and improve the efficacy of these algorithms, other teams have observed the highest tile values of 32768 <a name=\"stackoverflow\"></a>[<sup>[6]</sup>](#stackoverflownote). Another limitation is our computational power as we are unable to do a depth search past 4 with any reliability while the team that reached 32768 was able to use a depth search of 6 so the scale of improvements needed in our algorithm to match that performance is unknown. For future work our merge tiles heuristic didn’t perform well and we hope to see what improvements that can be made and its effects to our outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruction on running code\n",
    "\n",
    "Running the Expectimax Algorithm:\n",
    "\n",
    "- Clone the GitHub repository and ensure you are on the main branch.\n",
    "- Navigate to the 2048 folder.\n",
    "- Install all necessary libraries.\n",
    "- Execute the game by typing python main.py in your terminal.\n",
    "- Once the game interface opens, click on the 'ExpectiMax' button to start the game with the Expectimax algorithm.\n",
    "\n",
    "\n",
    "For running DQN or random algorithm:\n",
    "\n",
    "- With cloned github repo, switch to harry/merging/random-q\n",
    "- With the GitHub repository already cloned, switch to the harry/merging/random-q branch.\n",
    "- Navigate to the 2048 folder.\n",
    "- Install all required libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"nienote\"></a>1.[^](#nie): Nie, Yunn. AI Plays 2048, cs229.stanford.edu/proj2016/report/NieHouAn-AIPlays2048-report.pdf. Accessed 8 May 2024.<br> \n",
    "<a name=\"banerjeenote\"></a>2.[^](#banerjee): Banerjee, Rajit. “Rajitbanerjee/2048-Pygame: A Pygame Implementation of the Popular Single-Player Puzzle Game, 2048.” GitHub, github.com/rajitbanerjee/2048-pygame. Accessed 8 May 2024. <br>\n",
    "<a name=\"sonawanenote\"></a>3.[^](#sonawane): Sonawane, Ajinkya. “Solving 8-Puzzle Using A* Algorithm.” Medium, Good Audience, 24 June 2020, blog.goodaudience.com/solving-8-puzzle-using-a-algorithm-7b509c331288.<br>\n",
    "<a name=\"qwertnote\"></a>4.^: “Playing 2048 with Deep Q-Learning with PyTorch Implementation.” Medium, 15 April 2023, https://medium.com/@qwert12500/playing-2048-with-deep-q-learning-with-pytorch-implementation-4313291efe61. <br>\n",
    "<a name=\"tinkernote\"></a>5.^: “An Artificial Intelligence for 2048 Game.” Diary of a Tinker, 3 March 2014, https://diaryofatinker.blogspot.com/2014/03/an-artificial-intelligence-for-2048-game.html. <br>\n",
    "<a name=\"stackoverflownote\"></a>6.[^](#stackoverflow): \"What is the optimal algorithm for the game 2048?\" Stack Overflow, 17 March 2014, https://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048/22498940#22498940."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
